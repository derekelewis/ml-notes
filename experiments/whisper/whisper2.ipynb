{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import math\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = math.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = mx.exp(-log_timescale_increment * mx.arange(channels // 2))\n",
    "    scaled_time = mx.arange(length)[:, None] * inv_timescales[None, :]\n",
    "    return mx.concatenate([mx.sin(scaled_time), mx.cos(scaled_time)], axis=1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_state = n_state\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x,\n",
    "        xa=None,\n",
    "        mask=None,\n",
    "        kv_cache=None,\n",
    "    ):\n",
    "        q = self.query(x)\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        if xa is None:\n",
    "            k = self.key(x)\n",
    "            v = self.value(x)\n",
    "            if kv_cache is not None:\n",
    "                k = mx.concatenate([kv_cache[0], k], axis=1)\n",
    "                v = mx.concatenate([kv_cache[1], v], axis=1)\n",
    "        elif kv_cache is None:\n",
    "            k = self.key(xa)\n",
    "            v = self.value(xa)\n",
    "        else:\n",
    "            k, v = kv_cache\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), (k, v), qk\n",
    "\n",
    "    def qkv_attention(self, q, k, v, mask=None):\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.reshape(*q.shape[:2], self.n_head, -1).transpose(0, 2, 1, 3) * scale\n",
    "        k = k.reshape(*k.shape[:2], self.n_head, -1).transpose(0, 2, 3, 1) * scale\n",
    "        v = v.reshape(*v.shape[:2], self.n_head, -1).transpose(0, 2, 1, 3)\n",
    "\n",
    "        qk = q @ k\n",
    "        if mask is not None:\n",
    "            qk = qk + mask[:n_ctx, :n_ctx]\n",
    "        qk = qk.astype(mx.float32)\n",
    "\n",
    "        w = mx.softmax(qk, axis=-1).astype(q.dtype)\n",
    "        out = (w @ v).transpose(0, 2, 1, 3)\n",
    "        out = out.reshape(n_batch, n_ctx, n_state)\n",
    "        return out, qk\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = (\n",
    "            MultiHeadAttention(n_state, n_head) if cross_attention else None\n",
    "        )\n",
    "        self.cross_attn_ln = nn.LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp1 = nn.Linear(n_state, n_mlp)\n",
    "        self.mlp2 = nn.Linear(n_mlp, n_state)\n",
    "        self.mlp_ln = nn.LayerNorm(n_state)\n",
    "\n",
    "    def __call__(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        kv, cross_kv = kv_cache if kv_cache else (None, None)\n",
    "        print(\"kv_cache:\", kv_cache)\n",
    "        if kv_cache is not None:\n",
    "            print(\"kv_cache len:\", len(kv_cache))\n",
    "            print(\"kv_cache[0] len:\", len(kv_cache[0]))\n",
    "            print(\"kv_cache[1] len:\", len(kv_cache[1]))\n",
    "            print(\"kv_cache \")\n",
    "        y, kv, _ = self.attn(self.attn_ln(x), mask=mask, kv_cache=kv)\n",
    "        x += y\n",
    "        cross_qk = None\n",
    "        if self.cross_attn:\n",
    "            y, cross_kv, cross_qk = self.cross_attn(\n",
    "                self.cross_attn_ln(x), xa, kv_cache=cross_kv\n",
    "            )\n",
    "            x += y\n",
    "        x = x + self.mlp2(nn.gelu(self.mlp1(self.mlp_ln(x))))\n",
    "        return x, (kv, cross_kv), cross_qk\n",
    "    \n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mels: int,\n",
    "        n_ctx: int,\n",
    "        n_state: int,\n",
    "        n_head: int,\n",
    "        n_layer: int,\n",
    "        dtype: mx.Dtype = mx.float16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self._positional_embedding = sinusoids(n_ctx, n_state).astype(dtype)\n",
    "\n",
    "        self.blocks = [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n",
    "        self.ln_post = nn.LayerNorm(n_state)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        print(\"n_mels:\", self.n_mels)\n",
    "        print(\"n_ctx:\", self.n_ctx)\n",
    "        print(\"n_state:\", self.n_state)\n",
    "        print(\"n_head:\", self.n_head)\n",
    "        print(\"n_layer:\", self.n_layer)\n",
    "        print(\"x.shape:\", x.shape)\n",
    "        x = nn.gelu(self.conv1(x))\n",
    "        print(\"x.shape after conv1 & gelu:\", x.shape)\n",
    "        x = nn.gelu(self.conv2(x))\n",
    "        print(\"x.shape after conv2 & gelu:\", x.shape)\n",
    "        assert x.shape[1:] == self._positional_embedding.shape, \"incorrect audio shape\"\n",
    "        x = x + self._positional_embedding\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x, _, _ = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        return x\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab: int,\n",
    "        n_ctx: int,\n",
    "        n_state: int,\n",
    "        n_head: int,\n",
    "        n_layer: int,\n",
    "        dtype: mx.Dtype = mx.float16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = mx.zeros((n_ctx, n_state))\n",
    "\n",
    "        self.blocks = [\n",
    "            ResidualAttentionBlock(n_state, n_head, cross_attention=True)\n",
    "            for _ in range(n_layer)\n",
    "        ]\n",
    "        self.ln = nn.LayerNorm(n_state)\n",
    "        self._mask = nn.MultiHeadAttention.create_additive_causal_mask(n_ctx).astype(\n",
    "            dtype\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, xa, kv_cache=None):\n",
    "        \"\"\"\n",
    "        x : mx.array, shape = (batch_size, <= n_ctx)\n",
    "            the text tokens\n",
    "        xa : mx.array, shape = (batch_size, n_audio_ctx, n_audio_state)\n",
    "            the encoded audio features to be attended on\n",
    "        \"\"\"\n",
    "        offset = kv_cache[0][0][0].shape[1] if kv_cache else 0\n",
    "        x = (\n",
    "            self.token_embedding(x)\n",
    "            + self.positional_embedding[offset : offset + x.shape[-1]]\n",
    "        )\n",
    "\n",
    "        if kv_cache is None:\n",
    "            kv_cache = [None] * len(self.blocks)\n",
    "        cross_qk = [None] * len(self.blocks)\n",
    "        for e, block in enumerate(self.blocks):\n",
    "            x, kv_cache[e], cross_qk[e] = block(\n",
    "                x, xa, mask=self._mask, kv_cache=kv_cache[e]\n",
    "            )\n",
    "\n",
    "        x = self.ln(x)\n",
    "        return self.token_embedding.as_linear(x), kv_cache, cross_qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 80\n",
    "n_audio_ctx = 1500\n",
    "n_audio_state = 384\n",
    "n_audio_head = 6\n",
    "n_audio_layer = 4\n",
    "n_text_ctx = 448\n",
    "n_text_state = 384\n",
    "n_text_head = 6\n",
    "n_text_layer = 4\n",
    "n_vocab = 51865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(123)\n",
    "decoder = TextDecoder(n_vocab, n_text_ctx, n_text_state, n_text_head, n_text_layer, mx.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(123)\n",
    "audio_input_tensor = mx.random.normal([1, n_audio_ctx, n_audio_state])\n",
    "print(audio_input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(123)\n",
    "text_input_tensor = mx.random.randint(0,n_vocab,[1,1])\n",
    "print(text_input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, kv_cache, cross_qk = decoder(text_input_tensor, audio_input_tensor)\n",
    "print(\"embedding:\", embedding)\n",
    "print(\"kv_cache:\", kv_cache)\n",
    "print(\"cross_qk:\", cross_qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
